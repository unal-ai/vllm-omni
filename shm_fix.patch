From 9a65c507f29ba9e6e6f0f933e550fec9c6e29e12 Mon Sep 17 00:00:00 2001
From: ningzichun <ningzichun@connect.hku.hk>
Date: Tue, 20 Jan 2026 15:20:07 +0800
Subject: [PATCH] Fix shared memory broadcast hang in GLM-Image pipeline

- Add manual encoder activation support to SequentialOffloader
- Explicitly trigger vision_language_encoder onload before get_image_features in pipeline
- Prevents CPU-bound stalling during AR generation when offload is active
---
 .../models/glm_image/pipeline_glm_image.py       |  5 +++++
 vllm_omni/diffusion/offload.py                   | 16 +++++++++++++++-
 2 files changed, 20 insertions(+), 1 deletion(-)

diff --git a/vllm_omni/diffusion/models/glm_image/pipeline_glm_image.py b/vllm_omni/diffusion/models/glm_image/pipeline_glm_image.py
index 23a6807..10c12c2 100644
--- a/vllm_omni/diffusion/models/glm_image/pipeline_glm_image.py
+++ b/vllm_omni/diffusion/models/glm_image/pipeline_glm_image.py
@@ -478,6 +478,11 @@ class GlmImagePipeline(nn.Module):
         if image is not None and image_grid_thw is not None and len(image_grid_thw) > 1:
             # Get features only for condition images (exclude target image grid)
             condition_grid = image_grid_thw[:-1]
+
+            # Manually trigger offload if needed (as get_image_features doesn't trigger forward hooks)
+            if hasattr(self, "sequential_offloader"):
+                self.sequential_offloader.activate_encoder(self.vision_language_encoder)
+
             prior_token_image_embed = self.vision_language_encoder.get_image_features(
                 inputs["pixel_values"], condition_grid
             )
diff --git a/vllm_omni/diffusion/offload.py b/vllm_omni/diffusion/offload.py
index 5dc7c1f..337a20a 100644
--- a/vllm_omni/diffusion/offload.py
+++ b/vllm_omni/diffusion/offload.py
@@ -98,6 +98,16 @@ class SequentialOffloader:
         torch.cuda.synchronize()
         logger.debug("Swapped: DiT -> CPU, encoder -> GPU")
 
+    def activate_encoder(self, encoder: nn.Module) -> None:
+        """Manually activate an encoder (offload DiT, onload encoder).
+
+        Useful for methods that don't trigger forward pre-hooks (e.g. generate).
+        """
+        if encoder not in self.encoders:
+            return
+
+        self._encoder_pre_hook(encoder, ())
+
     def register(self) -> None:
         """Register forward pre-hooks on DiT and encoders."""
         # Hook on each DiT-like module
@@ -199,7 +209,11 @@ def apply_offload_hooks(
                     p.data = p.data.pin_memory()
 
     # Register sequential offload hooks
-    SequentialOffloader(dit_modules, encoders, device, pin).register()
+    offloader = SequentialOffloader(dit_modules, encoders, device, pin)
+    offloader.register()
+    
+    # Attach offloader to model for manual control
+    model.sequential_offloader = offloader
 
     logger.info(
         "CPU offload enabled: %s <-> %s (mutual exclusion)",
-- 
2.52.0

