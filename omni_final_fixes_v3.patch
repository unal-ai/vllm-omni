From 0c234b447e53c0a5682b986e17ccbfd43a577506 Mon Sep 17 00:00:00 2001
From: Your Name <you@example.com>
Date: Tue, 20 Jan 2026 12:05:14 +0800
Subject: [PATCH 1/4] Fix imports for vllm main compatibility

---
 vllm_omni/entrypoints/openai/api_server.py    | 17 ++++++----
 .../openai/protocol/chat_completion.py        |  2 +-
 vllm_omni/entrypoints/openai/serving_chat.py  | 34 +++++++++----------
 .../entrypoints/openai/serving_speech.py      |  2 +-
 4 files changed, 30 insertions(+), 25 deletions(-)

diff --git a/vllm_omni/entrypoints/openai/api_server.py b/vllm_omni/entrypoints/openai/api_server.py
index ff0e59b..0329837 100644
--- a/vllm_omni/entrypoints/openai/api_server.py
+++ b/vllm_omni/entrypoints/openai/api_server.py
@@ -25,19 +25,24 @@ from vllm.entrypoints.openai.api_server import (
     load_log_config,
     router,
     setup_server,
-    validate_json_request,
 )
-from vllm.entrypoints.openai.protocol import ChatCompletionRequest, ChatCompletionResponse, ErrorResponse
-from vllm.entrypoints.openai.serving_models import BaseModelPath, LoRAModulePath, OpenAIServingModels
-from vllm.entrypoints.openai.tool_parsers import ToolParserManager
+from vllm.entrypoints.openai.utils import validate_json_request
+from vllm.entrypoints.openai.chat_completion.protocol import (
+    ChatCompletionRequest,
+    ChatCompletionResponse,
+)
+from vllm.entrypoints.openai.engine.protocol import ErrorResponse
+from vllm.entrypoints.openai.models.protocol import BaseModelPath, LoRAModulePath
+from vllm.entrypoints.openai.models.serving import OpenAIServingModels
+from vllm.tool_parsers import ToolParserManager
 
 # yapf conflicts with isort for this block
 # yapf: disable
 # yapf: enable
-from vllm.entrypoints.tool_server import DemoToolServer, MCPToolServer, ToolServer
+from vllm.entrypoints.mcp.tool_server import DemoToolServer, MCPToolServer, ToolServer
 from vllm.entrypoints.utils import load_aware_call, with_cancellation
 from vllm.logger import init_logger
-from vllm.tokenizers import MistralTokenizer
+from vllm.tokenizers.mistral import MistralTokenizer
 from vllm.utils.system_utils import decorate_logs
 
 from vllm_omni.entrypoints.async_omni import AsyncOmni
diff --git a/vllm_omni/entrypoints/openai/protocol/chat_completion.py b/vllm_omni/entrypoints/openai/protocol/chat_completion.py
index 9f60762..cdc93f6 100644
--- a/vllm_omni/entrypoints/openai/protocol/chat_completion.py
+++ b/vllm_omni/entrypoints/openai/protocol/chat_completion.py
@@ -1,4 +1,4 @@
-from vllm.entrypoints.openai.protocol import ChatCompletionStreamResponse
+from vllm.entrypoints.openai.chat_completion.protocol import ChatCompletionStreamResponse
 
 
 class OmniChatCompletionStreamResponse(ChatCompletionStreamResponse):
diff --git a/vllm_omni/entrypoints/openai/serving_chat.py b/vllm_omni/entrypoints/openai/serving_chat.py
index 6fb9750..78b6820 100644
--- a/vllm_omni/entrypoints/openai/serving_chat.py
+++ b/vllm_omni/entrypoints/openai/serving_chat.py
@@ -29,8 +29,8 @@ from vllm.entrypoints.chat_utils import (
     make_tool_call_id,
     resolve_chat_template_content_format,
 )
-from vllm.entrypoints.harmony_utils import get_streamable_parser_for_assistant, parse_chat_output
-from vllm.entrypoints.openai.protocol import (
+from vllm.entrypoints.openai.parser.harmony_utils import get_streamable_parser_for_assistant, parse_chat_output
+from vllm.entrypoints.openai.chat_completion.protocol import (
     ChatCompletionNamedToolChoiceParam,
     ChatCompletionRequest,
     ChatCompletionResponse,
@@ -38,6 +38,8 @@ from vllm.entrypoints.openai.protocol import (
     ChatCompletionResponseStreamChoice,
     ChatCompletionStreamResponse,
     ChatMessage,
+)
+from vllm.entrypoints.openai.engine.protocol import (
     DeltaFunctionCall,
     DeltaMessage,
     DeltaToolCall,
@@ -49,18 +51,16 @@ from vllm.entrypoints.openai.protocol import (
     ToolCall,
     UsageInfo,
 )
-from vllm.entrypoints.openai.serving_chat import OpenAIServingChat
-from vllm.entrypoints.openai.serving_engine import (
+from vllm.entrypoints.openai.chat_completion.serving import OpenAIServingChat
+from vllm.entrypoints.openai.engine.serving import (
     ChatLikeRequest,
-    EngineTokensPrompt,
-    RequestPrompt,
-    ResponsesRequest,
-    TextTokensPrompt,
     clamp_prompt_logprobs,
-    is_list_of,
 )
-from vllm.entrypoints.openai.tool_parsers import ToolParser
-from vllm.entrypoints.openai.tool_parsers.mistral_tool_parser import MistralToolCall
+from vllm.entrypoints.openai.responses.protocol import ResponsesRequest
+from vllm.utils.collection_utils import is_list_of
+from vllm.inputs.data import TokensPrompt
+from vllm.tool_parsers import ToolParser
+from vllm.tool_parsers.mistral_tool_parser import MistralToolCall
 from vllm.entrypoints.openai.utils import maybe_filter_parallel_tool_calls
 from vllm.entrypoints.utils import should_include_usage
 from vllm.inputs.data import PromptType
@@ -318,8 +318,8 @@ class OmniOpenAIServingChat(OpenAIServingChat, AudioMixin):
         add_special_tokens: bool = False,
     ) -> tuple[
         list[ConversationMessage],
-        Sequence[RequestPrompt],
-        list[EngineTokensPrompt],
+        Sequence[PromptType],
+        list[TokensPrompt],
     ]:
         model_config = self.model_config
 
@@ -388,7 +388,7 @@ class OmniOpenAIServingChat(OpenAIServingChat, AudioMixin):
                 "Prompt has to be a string",
                 "when the tokenizer is not initialised",
             )
-            prompt_inputs = TextTokensPrompt(prompt=request_prompt, prompt_token_ids=[1])
+            prompt_inputs = TokensPrompt(prompt=request_prompt, prompt_token_ids=[1])
         elif isinstance(request_prompt, str):
             prompt_inputs = await self._tokenize_prompt_input_async(
                 request,
@@ -399,12 +399,12 @@ class OmniOpenAIServingChat(OpenAIServingChat, AudioMixin):
         else:
             # For MistralTokenizer
             assert is_list_of(request_prompt, int), "Prompt has to be either a string or a list of token ids"
-            prompt_inputs = TextTokensPrompt(
+            prompt_inputs = TokensPrompt(
                 prompt=tokenizer.decode(request_prompt),
                 prompt_token_ids=request_prompt,
             )
 
-        engine_prompt = EngineTokensPrompt(prompt_token_ids=prompt_inputs["prompt_token_ids"])
+        engine_prompt = TokensPrompt(prompt_token_ids=prompt_inputs["prompt_token_ids"])
         if mm_data is not None:
             engine_prompt["multi_modal_data"] = mm_data
 
@@ -513,7 +513,7 @@ class OmniOpenAIServingChat(OpenAIServingChat, AudioMixin):
     def _log_inputs(
         self,
         request_id: str,
-        inputs: RequestPrompt | PromptType,
+        inputs: PromptType,
         params_list: list[SamplingParams] | None,
         lora_request: LoRARequest | None,
     ) -> None:
diff --git a/vllm_omni/entrypoints/openai/serving_speech.py b/vllm_omni/entrypoints/openai/serving_speech.py
index c6b8781..77be4cc 100644
--- a/vllm_omni/entrypoints/openai/serving_speech.py
+++ b/vllm_omni/entrypoints/openai/serving_speech.py
@@ -2,7 +2,7 @@ import asyncio
 
 from fastapi import Request
 from fastapi.responses import Response
-from vllm.entrypoints.openai.serving_engine import OpenAIServing
+from vllm.entrypoints.openai.engine.serving import OpenAIServing
 from vllm.logger import init_logger
 from vllm.utils import random_uuid
 
-- 
2.34.1


From 5ed055468c0a1f05a9869d6110b70aa2c9fe95d7 Mon Sep 17 00:00:00 2001
From: Your Name <you@example.com>
Date: Tue, 20 Jan 2026 12:29:00 +0800
Subject: [PATCH 2/4] Fix CPU offload propagation and OOM issues in GLM-Image
 pipeline

---
 .../diffusion/models/glm_image/pipeline_glm_image.py | 12 +++++++++---
 vllm_omni/entrypoints/async_omni.py                  |  2 +-
 2 files changed, 10 insertions(+), 4 deletions(-)

diff --git a/vllm_omni/diffusion/models/glm_image/pipeline_glm_image.py b/vllm_omni/diffusion/models/glm_image/pipeline_glm_image.py
index 9a03a93..9b242fc 100644
--- a/vllm_omni/diffusion/models/glm_image/pipeline_glm_image.py
+++ b/vllm_omni/diffusion/models/glm_image/pipeline_glm_image.py
@@ -270,7 +270,9 @@ class GlmImagePipeline(nn.Module):
             subfolder="vision_language_encoder",
             local_files_only=True,
             torch_dtype=torch.bfloat16,
-        ).to(self.device)
+        )
+        if not od_config.enable_cpu_offload:
+            self.vision_language_encoder = self.vision_language_encoder.to(self.device)
         self.vision_language_encoder.eval()
 
         # Load processor for AR model
@@ -283,7 +285,9 @@ class GlmImagePipeline(nn.Module):
             subfolder="text_encoder",
             local_files_only=True,
             torch_dtype=torch.bfloat16,
-        ).to(self.device)
+        )
+        if not od_config.enable_cpu_offload:
+            self.text_encoder = self.text_encoder.to(self.device)
         self.text_encoder.eval()
 
         # Load tokenizer for glyph encoding
@@ -293,7 +297,9 @@ class GlmImagePipeline(nn.Module):
         logger.info("Loading AutoencoderKL (VAE)...")
         self.vae = AutoencoderKL.from_pretrained(
             model_path, subfolder="vae", local_files_only=True, torch_dtype=torch.bfloat16
-        ).to(self.device)
+        )
+        if not od_config.enable_cpu_offload:
+            self.vae = self.vae.to(self.device)
         self.vae.eval()
 
         # Load transformer (DiT)
diff --git a/vllm_omni/entrypoints/async_omni.py b/vllm_omni/entrypoints/async_omni.py
index 3c27514..98740d8 100644
--- a/vllm_omni/entrypoints/async_omni.py
+++ b/vllm_omni/entrypoints/async_omni.py
@@ -162,7 +162,7 @@ class AsyncOmni(OmniBase):
                     "vae_use_tiling": kwargs.get("vae_use_tiling", False),
                     "cache_backend": cache_backend,
                     "cache_config": cache_config,
-                    "enable_cpu_offload": kwargs.get("enable_cpu_offload", False),
+                    "enable_cpu_offload": kwargs.get("enable_cpu_offload", False) or (kwargs.get("cpu_offload_gb", 0) or 0) > 0,
                     "enforce_eager": kwargs.get("enforce_eager", False),
                 },
                 "final_output": True,
-- 
2.34.1


From f02c22dea428b281c45e8976db4666c161627748 Mon Sep 17 00:00:00 2001
From: Your Name <you@example.com>
Date: Tue, 20 Jan 2026 14:07:15 +0800
Subject: [PATCH 3/4] Include AR model in CPU offload hooks

---
 vllm_omni/diffusion/offload.py | 8 +++++++-
 1 file changed, 7 insertions(+), 1 deletion(-)

diff --git a/vllm_omni/diffusion/offload.py b/vllm_omni/diffusion/offload.py
index 4220ee9..5dc7c1f 100644
--- a/vllm_omni/diffusion/offload.py
+++ b/vllm_omni/diffusion/offload.py
@@ -171,7 +171,13 @@ def apply_offload_hooks(
     # Collect all encoders
     encoders: list[nn.Module] = []
     encoder_names: list[str] = []
-    for attr in ["text_encoder", "text_encoder_2", "text_encoder_3", "image_encoder"]:
+    for attr in [
+        "text_encoder",
+        "text_encoder_2",
+        "text_encoder_3",
+        "image_encoder",
+        "vision_language_encoder",
+    ]:
         if hasattr(model, attr) and getattr(model, attr) is not None:
             encoders.append(getattr(model, attr))
             encoder_names.append(attr)
-- 
2.34.1


From c961d5d16af959e6485c6193f687b7d912e18d4f Mon Sep 17 00:00:00 2001
From: Your Name <you@example.com>
Date: Tue, 20 Jan 2026 14:42:17 +0800
Subject: [PATCH 4/4] Fix device mismatch in generate_prior_tokens during CPU
 offload

---
 vllm_omni/diffusion/models/glm_image/pipeline_glm_image.py | 2 +-
 1 file changed, 1 insertion(+), 1 deletion(-)

diff --git a/vllm_omni/diffusion/models/glm_image/pipeline_glm_image.py b/vllm_omni/diffusion/models/glm_image/pipeline_glm_image.py
index 9b242fc..23a6807 100644
--- a/vllm_omni/diffusion/models/glm_image/pipeline_glm_image.py
+++ b/vllm_omni/diffusion/models/glm_image/pipeline_glm_image.py
@@ -442,7 +442,7 @@ class GlmImagePipeline(nn.Module):
             Tuple of (prior_token_ids, prior_token_image_ids)
             prior_token_image_ids is a list of tensors, one per condition image
         """
-        device = self.vision_language_encoder.device
+        device = self.device
         height = (height // factor) * factor
         width = (width // factor) * factor
         is_text_to_image = image is None or len(image) == 0
-- 
2.34.1
